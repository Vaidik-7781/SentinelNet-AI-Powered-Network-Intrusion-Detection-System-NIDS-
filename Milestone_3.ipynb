{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "Q60p4wAHDrmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV4LhzsAADl6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Wednesday-workingHours.pcap_ISCX.csv\")"
      ],
      "metadata": {
        "id": "b_tR2wgIAVOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "RAA7ES_QAYDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "26PiQsNtAYkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "ssiJ5QTCAebW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "wqGqgfxuAfPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "fsefqws_AhtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "G1-sHZ6CAkoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.heatmap(df.isnull(), cbar=False)\n",
        "plt.title(\"Missing Values Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MAqAqfrsAqtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of duplicate rows: {df.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "bJS9KSySArrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "wcli4JNFAu9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will show you the number of unique values in each column\n",
        "print(df.nunique())"
      ],
      "metadata": {
        "id": "vVdUs6i3AyQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "N5gizNGhA1uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping irrelevant features\n",
        "df.columns = df.columns.str.strip()\n",
        "irrelevant_cols = [\n",
        "    'Flow Bytes/s', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate',\n",
        "    'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate'\n",
        "]\n",
        "df = df.drop(columns=irrelevant_cols)"
      ],
      "metadata": {
        "id": "8vdXXyoqA4cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "7_4YYGJTA7zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving Cleaned Data\n",
        "df.to_csv(\"Wednesday_cleaned.csv\", index=False)"
      ],
      "metadata": {
        "id": "PEeZEu6tA_GU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select only numeric columns\n",
        "numeric_df = df.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(numeric_df.corr(), cmap=\"coolwarm\", annot=False)\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3lSTFX7pBB1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "numeric_df = df.select_dtypes(include=['int64', 'float64'])\n",
        "target_corr = numeric_df.corr()['Flow Duration'].sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "target_corr.plot(kind='bar', color='skyblue')\n",
        "plt.title(\"Correlation with Flow Duration\")\n",
        "plt.ylabel(\"Correlation Coefficient\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PS2MS0meBExh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=df['Flow Duration'])\n",
        "plt.title(\"Outlier Detection - Flow Duration\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KgXJ4F0xBH1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "z_scores = np.abs(stats.zscore(numeric_df))\n",
        "outliers = np.where(z_scores > 3)\n",
        "print(\"Outliers:\", len(outliers[0]))\n"
      ],
      "metadata": {
        "id": "uueca4LDBLLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = df.select_dtypes(include=['object']).columns\n",
        "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "print(\"Categorical:\", categorical_features)\n",
        "print(\"Numerical:\", numerical_features)"
      ],
      "metadata": {
        "id": "NMriN2VlBPMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['Label'] = le.fit_transform(df['Label'])\n",
        "print(df['Label'].unique())"
      ],
      "metadata": {
        "id": "weW4I8z4BSHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace inf/-inf with NaN\n",
        "df[numerical_features] = df[numerical_features].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# Optionally drop rows with NaN\n",
        "df[numerical_features] = df[numerical_features].dropna()"
      ],
      "metadata": {
        "id": "bA2j3viTBU-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally drop rows with NaN\n",
        "df[numerical_features] = df[numerical_features].dropna()\n",
        "\n",
        "for col in numerical_features:\n",
        "    col_data = df[col].replace([np.inf, -np.inf], np.nan).dropna()\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.hist(col_data, bins=30)\n",
        "    plt.title(f\"Histogram - {col}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Vu4OfTPYBXwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "important_features = [\n",
        "    'Flow Duration',\n",
        "    'Total Fwd Packets',\n",
        "    'Total Backward Packets',\n",
        "    'Fwd Packet Length Mean',\n",
        "    'Bwd Packet Length Mean',\n",
        "    'Fwd IAT Mean',\n",
        "    'Bwd IAT Mean',\n",
        "    'Flow Bytes/s',\n",
        "    'Label'\n",
        "]"
      ],
      "metadata": {
        "id": "ojRySsb3Baxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#WEEK 3\n",
        "# === Feature Engineering and Selection ===\n",
        "\n",
        "# Correlation threshold method\n",
        "corr_matrix = df[numerical_features].corr().abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
        "print(\"Highly correlated features to drop:\", to_drop)\n",
        "\n",
        "df_reduced = df.drop(columns=to_drop)\n",
        "\n",
        "# Feature importance using Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "X = df_reduced.drop('Label', axis=1)\n",
        "y = df_reduced['Label']\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "plt.figure(figsize=(10,6))\n",
        "importances[:15].plot(kind='bar')\n",
        "plt.title(\"Top 15 Important Features\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "i0mZnudnBdyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === WEEK 4: Supervised Model Training ===\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# --- Data Split ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# --- Handle Missing Values ---\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train = imputer.fit_transform(X_train)\n",
        "X_test = imputer.transform(X_test)\n",
        "\n",
        "# --- Feature Scaling ---\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# --- Random Forest Classifier ---\n",
        "print(\"üîπ Training Random Forest...\")\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "print(\"\\nRandom Forest Report:\\n\", classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# --- Support Vector Machine (SVM) ---\n",
        "# ‚ö° Tip: SVM can be slow for large datasets. Use a subset if needed.\n",
        "print(\"üîπ Training SVM (RBF Kernel)...\")\n",
        "if len(X_train) > 20000:  # if dataset is huge, train on subset\n",
        "    X_train_svm = X_train[:20000]\n",
        "    y_train_svm = y_train[:20000]\n",
        "else:\n",
        "    X_train_svm = X_train\n",
        "    y_train_svm = y_train\n",
        "\n",
        "svm = SVC(kernel='rbf', gamma='scale', C=1.0)\n",
        "svm.fit(X_train_svm, y_train_svm)\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "print(\"\\nSVM Report:\\n\", classification_report(y_test, y_pred_svm))\n",
        "\n",
        "# --- Logistic Regression ---\n",
        "print(\"üîπ Training Logistic Regression...\")\n",
        "lr = LogisticRegression(max_iter=500, solver='lbfgs', n_jobs=-1)\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "print(\"\\nLogistic Regression Report:\\n\", classification_report(y_test, y_pred_lr))\n",
        "\n",
        "print(\"\\n‚úÖ Training Completed Successfully!\")\n"
      ],
      "metadata": {
        "id": "1EMqQhTlB1T9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === WEEK 5: Anomaly Detection ===\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Handle Missing Values ---\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "# --- Feature Scaling (important for KMeans and IsolationForest) ---\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "# --- Isolation Forest for Anomaly Detection ---\n",
        "print(\"üîπ Running Isolation Forest...\")\n",
        "iso_forest = IsolationForest(contamination=0.02, random_state=42, n_jobs=-1)\n",
        "anomalies = iso_forest.fit_predict(X_scaled)\n",
        "\n",
        "# Add results to DataFrame\n",
        "df['Anomaly_IF'] = anomalies\n",
        "print(\"\\nIsolation Forest Anomaly Counts:\")\n",
        "print(df['Anomaly_IF'].value_counts())\n",
        "\n",
        "# Replace -1 (anomalies) and 1 (normal) with labels for readability\n",
        "df['Anomaly_IF_Label'] = df['Anomaly_IF'].map({1: 'Normal', -1: 'Anomaly'})\n",
        "\n",
        "# --- K-Means Clustering ---\n",
        "print(\"\\nüîπ Running K-Means Clustering...\")\n",
        "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
        "clusters = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Add results to DataFrame\n",
        "df['Cluster'] = clusters\n",
        "\n",
        "# --- Visualization ---\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(\n",
        "    x=X_scaled[:, 0],\n",
        "    y=X_scaled[:, 1],\n",
        "    hue=df['Cluster'],\n",
        "    palette='viridis',\n",
        "    s=10\n",
        ")\n",
        "plt.title(\"K-Means Clustering on Network Data\")\n",
        "plt.xlabel(\"Feature 1 (scaled)\")\n",
        "plt.ylabel(\"Feature 2 (scaled)\")\n",
        "plt.show()\n",
        "\n",
        "# --- Optional: Show anomaly percentage ---\n",
        "total = len(df)\n",
        "anomalies_count = (df['Anomaly_IF'] == -1).sum()\n",
        "print(f\"\\n‚ö†Ô∏è Detected {anomalies_count} anomalies out of {total} samples \"\n",
        "      f\"({(anomalies_count / total * 100):.2f}%).\")\n",
        "\n",
        "print(\"\\n‚úÖ Anomaly Detection Completed Successfully!\")\n"
      ],
      "metadata": {
        "id": "dIVtttlXB6e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WEEK 6\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [10, 20, None]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Score:\", grid.best_score_)\n",
        "\n",
        "# ROC Curve\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "y_prob = rf.predict_proba(X_test)[:,1]\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "plt.plot(fpr, tpr)\n",
        "plt.title(\"ROC Curve - Random Forest\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fmbFsXvWB9--"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
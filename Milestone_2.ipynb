{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "t0LHiVwjDpJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mERfn4Q8agg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Wednesday-workingHours.pcap_ISCX.csv\")"
      ],
      "metadata": {
        "id": "V7sAlV1M8uNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "QFWAkqXK8wC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "BW1uc_tm8x9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "IMm44S5P8zjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "EOFfhvyB81L5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "GcvaDVlr827S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "w10AZsDT84_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.heatmap(df.isnull(), cbar=False)\n",
        "plt.title(\"Missing Values Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "geqM3iz0864m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of duplicate rows: {df.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "YMcHK7fb89Gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "cLyu9v8u8-95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will show you the number of unique values in each column\n",
        "print(df.nunique())"
      ],
      "metadata": {
        "id": "E59NtA0A9MIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "P0uvI5gn9OUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping irrelevant features\n",
        "df.columns = df.columns.str.strip()\n",
        "irrelevant_cols = [\n",
        "    'Flow Bytes/s', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk', 'Fwd Avg Bulk Rate',\n",
        "    'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate'\n",
        "]\n",
        "df = df.drop(columns=irrelevant_cols)"
      ],
      "metadata": {
        "id": "Fs3gTfAs9R_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "-F7pS2T__ar8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving Cleaned Data\n",
        "df.to_csv(\"Wednesday_cleaned.csv\", index=False)"
      ],
      "metadata": {
        "id": "eN3Z0C5Q_e-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select only numeric columns\n",
        "numeric_df = df.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(numeric_df.corr(), cmap=\"coolwarm\", annot=False)\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7IavHfSB_gv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "numeric_df = df.select_dtypes(include=['int64', 'float64'])\n",
        "target_corr = numeric_df.corr()['Flow Duration'].sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "target_corr.plot(kind='bar', color='skyblue')\n",
        "plt.title(\"Correlation with Flow Duration\")\n",
        "plt.ylabel(\"Correlation Coefficient\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4v1muF3y_ilw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=df['Flow Duration'])\n",
        "plt.title(\"Outlier Detection - Flow Duration\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8bPDioxN_kY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "z_scores = np.abs(stats.zscore(numeric_df))\n",
        "outliers = np.where(z_scores > 3)\n",
        "print(\"Outliers:\", len(outliers[0]))\n"
      ],
      "metadata": {
        "id": "a9cjjmf0_l_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = df.select_dtypes(include=['object']).columns\n",
        "numerical_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "print(\"Categorical:\", categorical_features)\n",
        "print(\"Numerical:\", numerical_features)"
      ],
      "metadata": {
        "id": "qovijHka_nus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['Label'] = le.fit_transform(df['Label'])\n",
        "print(df['Label'].unique())"
      ],
      "metadata": {
        "id": "RByjUSr1_pyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace inf/-inf with NaN\n",
        "df[numerical_features] = df[numerical_features].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# Optionally drop rows with NaN\n",
        "df[numerical_features] = df[numerical_features].dropna()"
      ],
      "metadata": {
        "id": "rAsfVh5q_sTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optionally drop rows with NaN\n",
        "df[numerical_features] = df[numerical_features].dropna()\n",
        "\n",
        "for col in numerical_features:\n",
        "    col_data = df[col].replace([np.inf, -np.inf], np.nan).dropna()\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.hist(col_data, bins=30)\n",
        "    plt.title(f\"Histogram - {col}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "akh0kY6b_uSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "important_features = [\n",
        "    'Flow Duration',\n",
        "    'Total Fwd Packets',\n",
        "    'Total Backward Packets',\n",
        "    'Fwd Packet Length Mean',\n",
        "    'Bwd Packet Length Mean',\n",
        "    'Fwd IAT Mean',\n",
        "    'Bwd IAT Mean',\n",
        "    'Flow Bytes/s',\n",
        "    'Label'\n",
        "]"
      ],
      "metadata": {
        "id": "6xUJPvPj_wBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#WEEK 3\n",
        "# === Feature Engineering and Selection ===\n",
        "\n",
        "# Correlation threshold method\n",
        "corr_matrix = df[numerical_features].corr().abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
        "print(\"Highly correlated features to drop:\", to_drop)\n",
        "\n",
        "df_reduced = df.drop(columns=to_drop)\n",
        "\n",
        "# Feature importance using Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "X = df_reduced.drop('Label', axis=1)\n",
        "y = df_reduced['Label']\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "plt.figure(figsize=(10,6))\n",
        "importances[:15].plot(kind='bar')\n",
        "plt.title(\"Top 15 Important Features\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eQkxSC28Bzj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === WEEK 4: Supervised Model Training ===\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# --- Data Split ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# --- Handle Missing Values ---\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train = imputer.fit_transform(X_train)\n",
        "X_test = imputer.transform(X_test)\n",
        "\n",
        "# --- Feature Scaling ---\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# --- Random Forest Classifier ---\n",
        "print(\"ðŸ”¹ Training Random Forest...\")\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "print(\"\\nRandom Forest Report:\\n\", classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# --- Support Vector Machine (SVM) ---\n",
        "# âš¡ Tip: SVM can be slow for large datasets. Use a subset if needed.\n",
        "print(\"ðŸ”¹ Training SVM (RBF Kernel)...\")\n",
        "if len(X_train) > 20000:  # if dataset is huge, train on subset\n",
        "    X_train_svm = X_train[:20000]\n",
        "    y_train_svm = y_train[:20000]\n",
        "else:\n",
        "    X_train_svm = X_train\n",
        "    y_train_svm = y_train\n",
        "\n",
        "svm = SVC(kernel='rbf', gamma='scale', C=1.0)\n",
        "svm.fit(X_train_svm, y_train_svm)\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "print(\"\\nSVM Report:\\n\", classification_report(y_test, y_pred_svm))\n",
        "\n",
        "# --- Logistic Regression ---\n",
        "print(\"ðŸ”¹ Training Logistic Regression...\")\n",
        "lr = LogisticRegression(max_iter=500, solver='lbfgs', n_jobs=-1)\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "print(\"\\nLogistic Regression Report:\\n\", classification_report(y_test, y_pred_lr))\n",
        "\n",
        "print(\"\\nâœ… Training Completed Successfully!\")\n"
      ],
      "metadata": {
        "id": "P0UxLps7B5Ev"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}